{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A5fl51mcQfE0"
   },
   "source": [
    "ANLY 590 \\\n",
    "Final Project \\\n",
    "December 16, 2020 \\\n",
    "Random Forrest: Chris Fiaschetti, Ryan Callahan, Ruchikaa Kanar & Masha Gubenko"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x8L89NdpRARX"
   },
   "source": [
    "Github repo: https://github.com/mgubenko/590Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drive.mount('/content/gdrive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DdH-0VwUQSGK",
    "outputId": "4e91318e-6f05-4fb1-b229-5420dd2f3969"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as tfk\n",
    "import tensorflow.keras.layers as tfkl\n",
    "from tensorflow.keras.models import Sequential\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "from keras import layers\n",
    "\n",
    "\n",
    "#from google.colab import drive\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.io\n",
    "import os\n",
    "import PIL\n",
    "from PIL import Image\n",
    "from numpy import asarray\n",
    "#import cache_magic\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.applications import Xception\n",
    "from tensorflow.keras.applications import ResNet50V2\n",
    "from os import makedirs\n",
    "from tensorflow.keras.models import load_model\n",
    "from numpy import dstack\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.applications.efficientnet import EfficientNetB0 as Net\n",
    "import random\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DIHE0VC0QWPW",
    "outputId": "e3b86984-95aa-4314-8b14-c929230f0d6d"
   },
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "This portion of the code preprocesses the data. It combines the images data with its labels and produces train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "weS_LoQLEiNA",
    "outputId": "601ba9d6-7ed9-4ed3-beb6-d22baa9855a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n02097658-silky_terrier\n",
      "n02092002-Scottish_deerhound\n",
      "n02099849-Chesapeake_Bay_retriever\n",
      "n02091244-Ibizan_hound\n",
      "n02095314-wire-haired_fox_terrier\n",
      "n02091831-Saluki\n",
      "n02102318-cocker_spaniel\n",
      "n02104365-schipperke\n",
      "n02090622-borzoi\n",
      "n02113023-Pembroke\n",
      "n02105505-komondor\n",
      "n02093256-Staffordshire_bullterrier\n",
      "n02113799-standard_poodle\n",
      "n02109961-Eskimo_dog\n",
      "n02089973-English_foxhound\n",
      "n02099601-golden_retriever\n",
      "n02095889-Sealyham_terrier\n",
      "n02085782-Japanese_spaniel\n",
      "n02097047-miniature_schnauzer\n",
      "n02110063-malamute\n",
      "n02105162-malinois\n",
      "n02086079-Pekinese\n",
      "n02097130-giant_schnauzer\n",
      "n02113978-Mexican_hairless\n",
      "n02107142-Doberman\n",
      "n02097209-standard_schnauzer\n",
      "n02115913-dhole\n",
      "n02106662-German_shepherd\n",
      "n02106382-Bouvier_des_Flandres\n",
      "n02110185-Siberian_husky\n",
      "n02094258-Norwich_terrier\n",
      "n02093991-Irish_terrier\n",
      "n02094114-Norfolk_terrier\n",
      "n02109525-Saint_Bernard\n",
      "n02093754-Border_terrier\n",
      "n02105251-briard\n",
      "n02108551-Tibetan_mastiff\n",
      "n02108422-bull_mastiff\n",
      "n02085936-Maltese_dog\n",
      "n02093859-Kerry_blue_terrier\n",
      "n02104029-kuvasz\n",
      "n02107574-Greater_Swiss_Mountain_dog\n",
      "n02095570-Lakeland_terrier\n",
      "n02086646-Blenheim_spaniel\n",
      "n02088238-basset\n",
      "n02098286-West_Highland_white_terrier\n",
      "n02085620-Chihuahua\n",
      "n02106166-Border_collie\n",
      "n02090379-redbone\n",
      "n02090721-Irish_wolfhound\n",
      "n02088632-bluetick\n",
      "n02113712-miniature_poodle\n",
      "n02113186-Cardigan\n",
      "n02108000-EntleBucher\n",
      "n02091467-Norwegian_elkhound\n",
      "n02100236-German_short-haired_pointer\n",
      "n02107683-Bernese_mountain_dog\n",
      "n02086910-papillon\n",
      "n02097474-Tibetan_terrier\n",
      "n02101006-Gordon_setter\n",
      "n02093428-American_Staffordshire_terrier\n",
      "n02100583-vizsla\n",
      "n02105412-kelpie\n",
      "n02092339-Weimaraner\n",
      "n02107312-miniature_pinscher\n",
      "n02108089-boxer\n",
      "n02112137-chow\n",
      "n02105641-Old_English_sheepdog\n",
      "n02110958-pug\n",
      "n02087394-Rhodesian_ridgeback\n",
      "n02097298-Scotch_terrier\n",
      "n02086240-Shih-Tzu\n",
      "n02110627-affenpinscher\n",
      "n02091134-whippet\n",
      "n02102480-Sussex_spaniel\n",
      "n02091635-otterhound\n",
      "n02099267-flat-coated_retriever\n",
      "n02100735-English_setter\n",
      "n02091032-Italian_greyhound\n",
      "n02099712-Labrador_retriever\n",
      "n02106030-collie\n",
      "n02096177-cairn\n",
      "n02106550-Rottweiler\n",
      "n02096294-Australian_terrier\n",
      "n02087046-toy_terrier\n",
      "n02105855-Shetland_sheepdog\n",
      "n02116738-African_hunting_dog\n",
      "n02111277-Newfoundland\n",
      "n02089867-Walker_hound\n",
      "n02098413-Lhasa\n",
      "n02088364-beagle\n",
      "n02111889-Samoyed\n",
      "n02109047-Great_Dane\n",
      "n02096051-Airedale\n",
      "n02088466-bloodhound\n",
      "n02100877-Irish_setter\n",
      "n02112350-keeshond\n",
      "n02096437-Dandie_Dinmont\n",
      "n02110806-basenji\n",
      "n02093647-Bedlington_terrier\n",
      "n02107908-Appenzeller\n",
      "n02101556-clumber\n",
      "n02113624-toy_poodle\n",
      "n02111500-Great_Pyrenees\n",
      "n02102040-English_springer\n",
      "n02088094-Afghan_hound\n",
      "n02101388-Brittany_spaniel\n",
      "n02102177-Welsh_springer_spaniel\n",
      "n02096585-Boston_bull\n",
      "n02115641-dingo\n",
      "n02098105-soft-coated_wheaten_terrier\n",
      "n02099429-curly-coated_retriever\n",
      "n02108915-French_bulldog\n",
      "n02102973-Irish_water_spaniel\n",
      "n02112018-Pomeranian\n",
      "n02112706-Brabancon_griffon\n",
      "n02094433-Yorkshire_terrier\n",
      "n02105056-groenendael\n",
      "n02111129-Leonberg\n",
      "n02089078-black-and-tan_coonhound\n"
     ]
    }
   ],
   "source": [
    "# Define list that will hold all filenames in sequence\n",
    "file_string_list = []\n",
    "# Define list that will hold arrays of pictures' pixels in sequence\n",
    "data_list = []\n",
    "\n",
    "# Save string that is beginning of every image's filepath\n",
    "dirpath='Images/'\n",
    "# Save list of all folders of dog breeds\n",
    "folders = os.listdir(dirpath)\n",
    "# For each folder (i.e. breed)...\n",
    "for folder in folders:\n",
    "  # Print the folder (i.e. breed) name for tracking purposes\n",
    "  print(folder)\n",
    "  # Save current folder's path as separate string\n",
    "  folderpath = dirpath + folder + '/'\n",
    "  # Save all filenames within current folder to list\n",
    "  file_list = os.listdir(folderpath)\n",
    "  # For each file in this folder...\n",
    "  for filename in file_list:\n",
    "    # Save string that is that file's filepath\n",
    "    filepath = folderpath + filename\n",
    "    # Open the image at that filepath\n",
    "    image = Image.open(filepath)\n",
    "    # Resize image to 200x200 pixels to enable input into neural network\n",
    "    image_resized = image.resize((200,200))\n",
    "    # Convert this resized image to array\n",
    "    data = asarray(image_resized)\n",
    "    # Append this 200x200x3 array to list of all images' arrays\n",
    "    data_list.append(data)\n",
    "\n",
    "    # Save this image's folder and filename as string, which matches the folder-filename combination in the [file list-breed label] array below for later merger (i.e. we will be merging\n",
    "    # image arrays and breed labels using the filenames)\n",
    "    file_string = folder + '/' + filename\n",
    "    # Add this image's filename to list of all filenames, which will be in same sequence as list of all image data\n",
    "    file_string_list.append(file_string)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "btA-iaOpQZX7"
   },
   "outputs": [],
   "source": [
    "# Save strings of filepaths for files that contain images' filenames and associated labels\n",
    "file1 = \"file_list.mat\"\n",
    "file2 = \"test_list.mat\"\n",
    "file3 = \"train_list.mat\"\n",
    "## Read in files that contain images' filenames and associated labels\n",
    "mat1 = scipy.io.loadmat(file1)\n",
    "mat2 = scipy.io.loadmat(file2)\n",
    "mat3 = scipy.io.loadmat(file3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "D9kQP-RCdNOb"
   },
   "outputs": [],
   "source": [
    "# Save list of filenames of images in test set\n",
    "test_filenames_list = mat2['file_list']\n",
    "# Define list that will hold filenames in test set after they've been stripped of their triple-array packing\n",
    "test_filenames = []\n",
    "# For each filename in the test set...\n",
    "for filename in test_filenames_list:\n",
    "  # Strip off array wrapping and add to list of cleaned filenames\n",
    "  test_filenames.append(filename[0][0])\n",
    "# Convert filenames list to DataFrame column for concatenation\n",
    "test_filenames = pd.DataFrame(np.array(test_filenames).reshape((len(test_filenames),1)))\n",
    "# Convert labels list to DataFrame column for concatenation\n",
    "test_labels = pd.DataFrame(mat2['labels'])\n",
    "# Concatenate filenames and labels\n",
    "test_file_label = pd.concat((test_labels,test_filenames),axis=1)\n",
    "# Add column names to concatenated dataframe\n",
    "test_file_label.columns=['label','filename']\n",
    "\n",
    "# Save list of filenames of images in training set\n",
    "train_filenames_list = mat3['file_list']\n",
    "# Define list that will hold filenames in training set after they've been stripped of their triple-array packing\n",
    "train_filenames = []\n",
    "# For each filename in the training set...\n",
    "for filename in train_filenames_list:\n",
    "  # Strip off array wrapping and add to list of cleaned filenames\n",
    "  train_filenames.append(filename[0][0])\n",
    "# Convert filenames list to DataFrame column for concatenation\n",
    "train_filenames = pd.DataFrame(np.array(train_filenames).reshape((len(train_filenames),1)))\n",
    "# Convert labels list to DataFrame column for concatenation\n",
    "train_labels = pd.DataFrame(mat3['labels'])\n",
    "# Concatenate filenames and labels\n",
    "train_file_label = pd.concat((train_labels,train_filenames),axis=1)\n",
    "# Add column names to concatenated dataframe\n",
    "train_file_label.columns=['label','filename']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "6V65U5Ay5XX8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christopherfiaschetti/anaconda3/lib/python3.7/site-packages/pandas/core/internals/construction.py:230: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  values = np.array([convert(v) for v in values])\n"
     ]
    }
   ],
   "source": [
    "# Save all image arrays as dataframe\n",
    "a = pd.DataFrame(data_list)\n",
    "# Rename column of image arrays\n",
    "a = a.rename(columns={0:'data'})\n",
    "# Add filenames, which were acquired in sequence during original for loop, as column in dataframe\n",
    "a['filename'] = file_string_list\n",
    "# Create dataframe of training image arrays\n",
    "train_file = train_file_label.merge(a, on='filename', how='inner')\n",
    "# Create dataframe of test image arrays\n",
    "test_file = test_file_label.merge(a, on='filename', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "o06HNeeqArVk"
   },
   "outputs": [],
   "source": [
    "# Save list of training set labels\n",
    "y_train_numbers = train_file.label\n",
    "# Define array that will hold training set labels in softmax form\n",
    "y_train = np.zeros((y_train_numbers.shape[0],120))\n",
    "# For each image...\n",
    "for i in range(y_train_numbers.shape[0]):\n",
    "    # Flip the appropriate column's entry (each column represents a different breed) to 1\n",
    "    y_train[i,y_train_numbers[i]-1] = 1\n",
    "# Save list of test set labels\n",
    "y_test_numbers = test_file.label\n",
    "# Define array that will hold test set labels in softmax form\n",
    "y_test = np.zeros((y_test_numbers.shape[0],120))\n",
    "# For each image...\n",
    "for i in range(y_test_numbers.shape[0]):\n",
    "    # Flip the appropriate column's entry (each column represents a different breed) to 1\n",
    "    y_test[i,y_test_numbers[i]-1] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "_bK-ATtyA6Tk"
   },
   "outputs": [],
   "source": [
    "# Save series of image arrays as list\n",
    "X_train = list(train_file.data)\n",
    "# Remove empty 4th dimension from 7904th image\n",
    "X_train[7904] = X_train[7904][:,:,0:3]\n",
    "# Convert training set to numpy array and normalize variables\n",
    "X_train = np.array(X_train).astype('float32') / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert test set to numpy array and normalize variables\n",
    "X_test = np.array(list(test_file.data)).astype('float32') / 255."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "\n",
    "### CNN with 3x3 kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first two models are representative of the approximately 20 \"handmade\" models that we tested. We experimented with different combinations of padding, stride length, network depth hidden layer size, activation functions, etc... Spoiler alert is that none of these models significantly outperformed random guessing.\n",
    "\n",
    "This first model is very standard - 3x3 kernels throughout, strides of 1 in both axes throughout, ReLU activation functions, padding that preserves dimensions, and a neuron sequence of 32-32-64-64-128-128-32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "_bK-ATtyA6Tk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 200, 200, 32)      896       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 200, 200, 32)      9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 100, 100, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 100, 100, 64)      18496     \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 100, 100, 64)      36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 50, 50, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 50, 50, 128)       73856     \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 50, 50, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 25, 25, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 25, 25, 32)        36896     \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 20000)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 120)               2400120   \n",
      "=================================================================\n",
      "Total params: 2,724,024\n",
      "Trainable params: 2,724,024\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Start with empty sequential model\n",
    "model = Sequential()\n",
    "# Add pair of 32-neuron hidden layers with ReLU activation function, 3x3 kernel, stride length of 1 in both dimensions, and padding that maintains 32x32 shape\n",
    "model.add(tfkl.Conv2D(32, (3,3), strides=(1,1), activation='relu', padding='same', input_shape=(200, 200, 3)))\n",
    "model.add(tfkl.Conv2D(32, (3,3), strides=(1,1), activation='relu', padding='same'))\n",
    "# Max pooling of 2x2 sections to get dimensionality of each image down to 16x16\n",
    "model.add(tfkl.MaxPooling2D((2, 2)))\n",
    "# Add pair of 64-neuron hidden layers with ReLU activation function, 3x3 kernel, stride length of 1 in both dimensions, and padding that maintains 16x16 shape\n",
    "model.add(tfkl.Conv2D(64, (3,3), strides=(1,1), activation='relu', padding='same'))\n",
    "model.add(tfkl.Conv2D(64, (3,3), strides=(1,1), activation='relu', padding='same'))\n",
    "# Max pooling of 2x2 sections to get dimensionality of each image down to 8x8\n",
    "model.add(tfkl.MaxPooling2D((2, 2)))\n",
    "# Add pair of 128-neuron hidden layers with ReLU activation function, 3x3 kernel, stride length of 1 in both dimensions, and padding that maintains 8x8 shape\n",
    "model.add(tfkl.Conv2D(128, (3,3), strides=(1,1), activation='relu', padding='same'))\n",
    "model.add(tfkl.Conv2D(128, (3, 3), strides=(1,1), activation='relu', padding='same'))\n",
    "# Max pooling of 2x2 sections to get dimensionality of each image down to 4x4\n",
    "model.add(tfkl.MaxPooling2D((2, 2)))\n",
    "# Flatten 4x4x128 network into \n",
    "model.add(tfkl.Conv2D(32, (3, 3), strides=(1,1), activation='relu', padding='same'))\n",
    "model.add(tfkl.Flatten())\n",
    "model.add(tfkl.Dense(120, activation='softmax'))\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "5SWB-YMpA9YZ",
    "outputId": "f187824d-4f75-4054-89f3-46fddc0ada8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188/188 [==============================] - 677s 4s/step - loss: 4.7888 - accuracy: 0.0087 - val_loss: 4.7876 - val_accuracy: 0.0138\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fb13e188320>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=1, batch_size=64, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "269/269 [==============================] - 100s 370ms/step - loss: 4.7876 - accuracy: 0.0138\n",
      "Test set\n",
      "  Loss: 4.788\n",
      "  Accuracy: 0.014\n"
     ]
    }
   ],
   "source": [
    "#Calculate the validation accuracy levels to see how the model performed\n",
    "acc1 = model.evaluate(X_test, y_test)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(acc1[0],acc1[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN with 6x6 kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second model only changes the kernel size to 6x6; otherwise, we keep strides of 1 in both axes throughout, ReLU activation functions, padding that preserves dimensions, and a neuron sequence of 32-32-64-64-128-128-32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "s-mDOFvKRkfC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_7 (Conv2D)            (None, 200, 200, 32)      3488      \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 200, 200, 32)      36896     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 100, 100, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 100, 100, 64)      73792     \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 100, 100, 64)      147520    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 50, 50, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 50, 50, 128)       295040    \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 50, 50, 128)       589952    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 25, 25, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 25, 25, 32)        147488    \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 20000)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 120)               2400120   \n",
      "=================================================================\n",
      "Total params: 3,694,296\n",
      "Trainable params: 3,694,296\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Start with empty sequential model\n",
    "model2 = Sequential()\n",
    "# Add pair of 32-neuron hidden layers with ReLU activation function, 3x3 kernel, stride length of 1 in both dimensions, and padding that maintains 32x32 shape\n",
    "model2.add(tfkl.Conv2D(32, (6,6), strides=(1,1), activation='relu', padding='same', input_shape=(200, 200, 3)))\n",
    "model2.add(tfkl.Conv2D(32, (6,6), strides=(1,1), activation='relu', padding='same'))\n",
    "# Max pooling of 2x2 sections to get dimensionality of each image down to 16x16\n",
    "model2.add(tfkl.MaxPooling2D((2, 2)))\n",
    "# Add pair of 64-neuron hidden layers with ReLU activation function, 3x3 kernel, stride length of 1 in both dimensions, and padding that maintains 16x16 shape\n",
    "model2.add(tfkl.Conv2D(64, (6,6), strides=(1,1), activation='relu', padding='same'))\n",
    "model2.add(tfkl.Conv2D(64, (6,6), strides=(1,1), activation='relu', padding='same'))\n",
    "# Max pooling of 2x2 sections to get dimensionality of each image down to 8x8\n",
    "model2.add(tfkl.MaxPooling2D((2, 2)))\n",
    "# Add pair of 128-neuron hidden layers with ReLU activation function, 3x3 kernel, stride length of 1 in both dimensions, and padding that maintains 8x8 shape\n",
    "model2.add(tfkl.Conv2D(128, (6,6), strides=(1,1), activation='relu', padding='same'))\n",
    "model2.add(tfkl.Conv2D(128, (6,6), strides=(1,1), activation='relu', padding='same'))\n",
    "# Max pooling of 2x2 sections to get dimensionality of each image down to 4x4\n",
    "model2.add(tfkl.MaxPooling2D((2, 2)))\n",
    "# Flatten 4x4x128 network into \n",
    "model2.add(tfkl.Conv2D(32, (6,6), strides=(1,1), activation='relu', padding='same'))\n",
    "#model2.add(layers.Conv2D(16, (6,6), strides=(1,1), activation='relu', padding='same'))\n",
    "model2.add(tfkl.Flatten())\n",
    "model2.add(tfkl.Dense(120, activation='softmax'))\n",
    "model2.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188/188 [==============================] - 2120s 11s/step - loss: 4.7977 - accuracy: 0.0072 - val_loss: 4.7876 - val_accuracy: 0.0087\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fafb8026860>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.fit(X_train, y_train, epochs=1, batch_size=64, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "269/269 [==============================] - 282s 1s/step - loss: 4.7875 - accuracy: 0.0087\n",
      "Test set\n",
      "  Loss: 4.788\n",
      "  Accuracy: 0.009\n"
     ]
    }
   ],
   "source": [
    "#Calculate the validation accuracy levels to see how the model performed\n",
    "acc2 = model2.evaluate(X_test, y_test)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(acc2[0],acc2[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we said, we tried a large number of different \"handmade\" models, with all of them having a validation set accuracy of approximately 0.9% (equivalent to random guessing)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer Learning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After all of those \"handmade\" models failed, we used transfer learning models for this image classification task.\n",
    "\n",
    "#### VGG Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg16 (Functional)           (None, 6, 6, 512)         14714688  \n",
      "_________________________________________________________________\n",
      "global_max_pooling2d (Global (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 120)               61560     \n",
      "=================================================================\n",
      "Total params: 14,776,248\n",
      "Trainable params: 61,560\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Import library that contains VGG weights\n",
    "\n",
    "# Construct variable that uses VGG weights for 200x200x3 input image\n",
    "conv_base = VGG16(weights=\"imagenet\",include_top=False,input_shape=(200,200,3))\n",
    "# Define transfer learning model\n",
    "model3 = Sequential()\n",
    "# Freeze VGG weights\n",
    "conv_base.trainable=False\n",
    "# Add VGG weights to model\n",
    "model3.add(conv_base)\n",
    "# Add final dense 120-element layer that will contain softmax probabilities\n",
    "model3.add(tfkl.GlobalMaxPool2D())\n",
    "model3.add(tfkl.Dense(120,activation='softmax'))\n",
    "# Compile model\n",
    "model3.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['categorical_accuracy'])\n",
    "# Print model summary\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "750/750 [==============================] - 1361s 2s/step - loss: 4.8512 - categorical_accuracy: 0.0330 - val_loss: 3.8737 - val_categorical_accuracy: 0.1284\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fb00a9bed30>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model\n",
    "model3.fit(X_train, y_train, epochs=1, batch_size=16, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "269/269 [==============================] - 558s 2s/step - loss: 3.8737 - categorical_accuracy: 0.1284\n",
      "Test set\n",
      "  Loss: 3.874\n",
      "  Accuracy: 0.128\n"
     ]
    }
   ],
   "source": [
    "#Calculate the validation accuracy levels to see how the model performed\n",
    "acc3 = model3.evaluate(X_test, y_test)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(acc3[0],acc3[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Xception Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "xception (Functional)        (None, 7, 7, 2048)        20861480  \n",
      "_________________________________________________________________\n",
      "global_max_pooling2d_1 (Glob (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 120)               245880    \n",
      "=================================================================\n",
      "Total params: 21,107,360\n",
      "Trainable params: 245,880\n",
      "Non-trainable params: 20,861,480\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Import library that contains Xception weights\n",
    "\n",
    "# Construct variable that uses Xception weights for 200x200x3 input image\n",
    "conv_base = Xception(weights=\"imagenet\",include_top=False,input_shape=(200,200,3))\n",
    "# Define transfer learning model\n",
    "model4 = Sequential()\n",
    "# Freeze Xception weights\n",
    "conv_base.trainable=False\n",
    "# Add Xception weights to model\n",
    "model4.add(conv_base)\n",
    "# Add final dense 120-element layer that will contain softmax probabilities\n",
    "model4.add(tfkl.GlobalMaxPool2D())\n",
    "model4.add(tfkl.Dense(120,activation='softmax'))\n",
    "# Compile model\n",
    "model4.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['categorical_accuracy'])\n",
    "# Print model summary\n",
    "model4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "375/375 [==============================] - 703s 2s/step - loss: 2.3424 - categorical_accuracy: 0.5390 - val_loss: 1.2336 - val_categorical_accuracy: 0.7204\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fafb80af240>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model\n",
    "model4.fit(X_train, y_train, epochs=1, batch_size=32, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "269/269 [==============================] - 297s 1s/step - loss: 1.2336 - categorical_accuracy: 0.7204\n",
      "Test set\n",
      "  Loss: 1.234\n",
      "  Accuracy: 0.720\n"
     ]
    }
   ],
   "source": [
    "#Calculate the validation accuracy levels to see how the model performed\n",
    "acc4 = model4.evaluate(X_test, y_test)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(acc4[0],acc4[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ResNet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "resnet50v2 (Functional)      (None, 7, 7, 2048)        23564800  \n",
      "_________________________________________________________________\n",
      "global_max_pooling2d_2 (Glob (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 120)               245880    \n",
      "=================================================================\n",
      "Total params: 23,810,680\n",
      "Trainable params: 245,880\n",
      "Non-trainable params: 23,564,800\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Construct variable that uses ResNet weights for 200x200x3 input image\n",
    "conv_base = ResNet50V2(weights=\"imagenet\",include_top=False,input_shape=(200,200,3))\n",
    "# Define transfer learning model\n",
    "model5 = Sequential()\n",
    "# Freeze ResNet weights\n",
    "conv_base.trainable=False\n",
    "# Add ResNet weights to model\n",
    "model5.add(conv_base)\n",
    "# Add final dense 120-element layer that will contain softmax probabilities\n",
    "model5.add(tfkl.GlobalMaxPool2D())\n",
    "model5.add(tfkl.Dense(120,activation='softmax'))\n",
    "# Compile model\n",
    "model5.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['categorical_accuracy'])\n",
    "# Print model summary\n",
    "model5.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "375/375 [==============================] - 538s 1s/step - loss: 13.2497 - categorical_accuracy: 0.2688 - val_loss: 7.2558 - val_categorical_accuracy: 0.5052\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fb00af0d748>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model5.fit(X_train, y_train, epochs=1, batch_size=32, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "269/269 [==============================] - 221s 822ms/step - loss: 7.2558 - categorical_accuracy: 0.5052\n",
      "Test set\n",
      "  Loss: 7.256\n",
      "  Accuracy: 0.505\n"
     ]
    }
   ],
   "source": [
    "#Calculate the validation accuracy levels to see how the model performed\n",
    "acc5 = model5.evaluate(X_test, y_test)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(acc5[0],acc5[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Efficient Net Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct variable that uses Efficient Net weights for 200x200x3 input image\n",
    "efnet = Net(weights = \"imagenet\", include_top = False, input_shape = (200,200,3))\n",
    "# Freeze Efficiet Net weights\n",
    "efnet.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "efficientnetb0 (Functional)  (None, 7, 7, 1280)        4049571   \n",
      "_________________________________________________________________\n",
      "gap (GlobalMaxPooling2D)     (None, 1280)              0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 1280)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 120)               153720    \n",
      "=================================================================\n",
      "Total params: 4,203,291\n",
      "Trainable params: 153,720\n",
      "Non-trainable params: 4,049,571\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define transfer learning model\n",
    "mnet = Sequential()\n",
    "# Add Efficient Net weights to model\n",
    "mnet.add(efnet)\n",
    "# Add final dense 120-element layer that will contain softmax probabilities\n",
    "mnet.add(tfkl.GlobalMaxPooling2D(name = \"gap\"))\n",
    "mnet.add(tfkl.Flatten())\n",
    "mnet.add(tfkl.Dense(120, activation='softmax'))\n",
    "# Compile model\n",
    "mnet.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "mnet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 175s 170s/step - loss: 9.1571 - accuracy: 0.0156 - val_loss: 9.0473 - val_accuracy: 0.0066\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fb1190af4a8>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fit the model\n",
    "mnet.fit(X_train, y_train, epochs=1, steps_per_epoch= 2, batch_size=64, validation_data = (X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "269/269 [==============================] - 147s 546ms/step - loss: 9.0473 - accuracy: 0.0066\n",
      "Test set\n",
      "  Loss: 9.047\n",
      "  Accuracy: 0.007\n"
     ]
    }
   ],
   "source": [
    "#Calculate the validation accuracy levels to see how the model performed\n",
    "acc_ts = mnet.evaluate(X_test, y_test)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(acc_ts[0],acc_ts[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the relative performance of each of these standalone models, we see that the Xception embeddings performed best (72% test set accuracy), followed by ResNet (50.5% test set accuracy), followed by VGG (12.8% accuracy), and finally Efficient Net (equivalent to random guessing). This is despite the fact that VGG took nearly twice as long as Xception, and more than twice as long as ResNet, to train this single epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Models\n",
    "\n",
    "As the meat of our project, we used ensemble models for image classification, to see if we could improve upon the performance of the standalone models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ensembles of Transfer Learning Models\n",
    "\n",
    "We will now try to ensemble combinations of transfer learning models. First, we make an ensemble of Xception, ResNet, and VGG models, the three best-performing models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Xception model that was trained above\n",
    "model4.save('models/model_4.h5')\n",
    "# Save ResNet model that was trained above\n",
    "model5.save('models/model_5.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "269/269 [==============================] - 284s 1s/step - loss: 1.2336 - categorical_accuracy: 0.7204\n",
      "269/269 [==============================] - 218s 809ms/step - loss: 7.2558 - categorical_accuracy: 0.5052\n",
      "Xception accuracy: [1.2335715293884277, 0.7203962802886963]\n",
      "ResNet accuracy: [7.255791664123535, 0.5052447319030762]\n"
     ]
    }
   ],
   "source": [
    "# Define empty list that will hold transfer sub-models\n",
    "all_tran_models = list()\n",
    "# Load Xception model\n",
    "model4 = load_model('models/model_4.h5')\n",
    "# Add Xception model to list\n",
    "all_tran_models.append(model4)\n",
    "# Load ResNet model\n",
    "model5 = load_model('models/model_5.h5')\n",
    "# Add ResNet model to list\n",
    "all_tran_models.append(model5)\n",
    "\n",
    "# Evaluate and print accuracy of Xception and ResNet models in isolation (for comparison against ensemble accuracy)\n",
    "model4_acc = model4.evaluate(X_test,y_test)\n",
    "model5_acc = model5.evaluate(X_test,y_test)\n",
    "print('Xception accuracy:',model4_acc)\n",
    "print('ResNet accuracy:',model5_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "269/269 [==============================] - 528s 2s/step - loss: 3.8737 - categorical_accuracy: 0.1284\n",
      "VGG accuracy: [3.8736929893493652, 0.12843823432922363]\n"
     ]
    }
   ],
   "source": [
    "# Save VGG model to file\n",
    "model3.save('models/model_3.h5')\n",
    "# Load VGG model from disk\n",
    "model3 = load_model('models/model_3.h5')\n",
    "# Add VGG model to list of models\n",
    "all_tran_models.append(model3)\n",
    "# Evaluate and print performance of VGG model in isolation\n",
    "model3_acc = model3.evaluate(X_test,y_test)\n",
    "print('VGG accuracy:',model3_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define function that makes predictions based on fit ensemble model\n",
    "def stacked_prediction(members, model, inputX):\n",
    "    # Create stacked copies of dataset\n",
    "    stackedX = stacked_dataset(members, inputX)\n",
    "    # Make a prediction on stacked, copied dataset using ensemble model\n",
    "    yhat = model.predict(stackedX)\n",
    "    return yhat\n",
    "\n",
    "## Function creates multiple copies of X for evaluation\n",
    "def stacked_dataset(members, inputX):\n",
    "    # Define structure that will hold all predictions\n",
    "    stackX = None\n",
    "    # For each model contributing to ensemble...\n",
    "    for model in members:\n",
    "        # Make predictions using this model\n",
    "        yhat = model.predict(inputX)\n",
    "        # If stack is empty, start stack with predictions from this model\n",
    "        if stackX is None:\n",
    "            stackX = yhat\n",
    "        # If stack is not empty, add predictions from this model to stack\n",
    "        else:\n",
    "            stackX = dstack((stackX, yhat))\n",
    "    # Flatten predictions of all models into [observations, models x probabilities] shape\n",
    "    stackX = stackX.reshape((stackX.shape[0], stackX.shape[1]*stackX.shape[2]))\n",
    "    # Return flattened predictions\n",
    "    return stackX\n",
    "\n",
    "# Define function that fits ensemble model\n",
    "def fit_stacked_model(members, inputX, inputy):\n",
    "    # Create dataset of multiple copies of datset for use in ensemble model\n",
    "    stackedX = stacked_dataset(members, inputX)\n",
    "    # Fit ensemble model\n",
    "    model = LogisticRegression()\n",
    "    model.fit(stackedX, inputy)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christopherfiaschetti/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/christopherfiaschetti/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacked Test Accuracy: 0.771\n"
     ]
    }
   ],
   "source": [
    "# Use 2 aforementioned functions to create dataset shape to fit ensemble model, and then fit ensemble model\n",
    "model_ensemble = fit_stacked_model(all_tran_models, X_test, np.array(y_test_numbers))\n",
    "\n",
    "\n",
    "# Make predictions using fit ensemble model\n",
    "yhat = stacked_prediction(all_tran_models, model_ensemble, X_test)\n",
    "# Print accuracy of ensemble model\n",
    "acc = accuracy_score(np.array(y_test_numbers), yhat)\n",
    "print('Stacked Test Accuracy: %.3f' % acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We achieved a test accuracy of 77.1% with an ensemble model that included VGG-, ResNet-, and Xception-informed standalone models, which is better than any of those models' performance in isolation. Now we will try to add an Efficient Net model to this ensemble to see if it further improves its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "269/269 [==============================] - 154s 566ms/step - loss: 9.0473 - accuracy: 0.0066\n",
      "Efficient Net Accuracy: [9.047321319580078, 0.006643356755375862]\n"
     ]
    }
   ],
   "source": [
    "# Save Efficient Net transfer learning model\n",
    "mnet.save('models/emnet.h5')\n",
    "# Load Efficient Net transfer learning model\n",
    "model6 = load_model('models/emnet.h5')\n",
    "# Add Efficient Net model to list of all models\n",
    "all_tran_models.append(model6)\n",
    "# Evaluate Efficient Net's standalone accuracy\n",
    "model6_acc = model6.evaluate(X_test,y_test)\n",
    "print('Efficient Net Accuracy:',model6_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christopherfiaschetti/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/christopherfiaschetti/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacked Test Accuracy: 0.771\n"
     ]
    }
   ],
   "source": [
    "# Fit ensemble model that now includes fourth transfer model, the Efficient Net\n",
    "model_ensemble2 = fit_stacked_model(all_tran_models, X_test, np.array(y_test_numbers))\n",
    "# Make predictions using new (four-piece) ensemble model\n",
    "yhat2 = stacked_prediction(all_tran_models, model_ensemble2, X_test)\n",
    "# Print accuracy of new ensemble model\n",
    "acc2 = accuracy_score(np.array(y_test_numbers), yhat2)\n",
    "print('Stacked Test Accuracy: %.3f' % acc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alas, the addition of the Efficient Net model did not further improve the ensemble's performance, though that is not especially surprising because the Efficient Net model's standalone test accuracy was barely better than random guessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ensemble of Simple Models\n",
    "\n",
    "We also constructed an ensemble of a set of simple \"handmade\" models to see if we can outperform our standalone \"handmade\" models from above (which could not outperform random guessing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define function that defines, compiles, fits, and returns a simple model\n",
    "def fit_model(trainX, trainy):\n",
    "    # define model\n",
    "    model_little = Sequential()\n",
    "    model_little.add(tfkl.Conv2D(32, (5,5), strides=(1,1), activation='relu', padding='same', input_shape=(200, 200, 3)))\n",
    "    model_little.add(tfkl.MaxPooling2D((4, 4)))\n",
    "    model_little.add(tfkl.Conv2D(16, (5,5), strides=(1,1), activation='relu', padding='same'))\n",
    "    model_little.add(tfkl.MaxPooling2D((2, 2)))\n",
    "    model_little.add(tfkl.Conv2D(8, (3,3), strides=(1,1), activation='relu', padding='same'))\n",
    "    model_little.add(tfkl.MaxPooling2D((2, 2)))\n",
    "    model_little.add(tfkl.Conv2D(4, (3,3), strides=(1,1), activation='relu', padding='same'))\n",
    "    model_little.add(tfkl.MaxPooling2D((2, 2)))\n",
    "    model_little.add(tfkl.Flatten())\n",
    "    model_little.add(tfkl.Dense(120, activation='softmax'))\n",
    "    model_little.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model_little.fit(X_train, y_train, epochs=5)\n",
    "    return model_little\n",
    "\n",
    "## Define function that loads a (simple) model and adds that model to the list of all models\n",
    "def load_all_models(n_models):\n",
    "    all_models = list()\n",
    "    for i in range(n_models):\n",
    "        # define filename for this ensemble\n",
    "        filename = 'models/model___' + str(i + 1) + '.h5'\n",
    "        # load model from file\n",
    "        model = load_model(filename)\n",
    "        # add to list of members\n",
    "        all_models.append(model)\n",
    "        print('>loaded %s' % filename)\n",
    "    return all_models\n",
    "\n",
    "\n",
    "## Define function that loads a (simple) model and adds that model to the list of all models\n",
    "def load_all_sub_models(n_models):\n",
    "    all_models = list()\n",
    "    for i in range(n_models):\n",
    "        # define filename for this ensemble\n",
    "        filename = 'models/model_sub_' + str(i + 1) + '.h5'\n",
    "        # load model from file\n",
    "        model = load_model(filename)\n",
    "        # add to list of members\n",
    "        all_models.append(model)\n",
    "        print('>loaded %s' % filename)\n",
    "    return all_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will create an ensemble model out of 5 individual models that were each trained on the entire training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "375/375 [==============================] - 142s 379ms/step - loss: 4.7884 - accuracy: 0.0077\n",
      "Epoch 2/5\n",
      "375/375 [==============================] - 144s 383ms/step - loss: 4.7851 - accuracy: 0.0086\n",
      "Epoch 3/5\n",
      "375/375 [==============================] - 143s 381ms/step - loss: 4.6693 - accuracy: 0.0250\n",
      "Epoch 4/5\n",
      "375/375 [==============================] - 143s 382ms/step - loss: 4.5108 - accuracy: 0.0375\n",
      "Epoch 5/5\n",
      "375/375 [==============================] - 144s 383ms/step - loss: 4.4113 - accuracy: 0.0493\n",
      ">Saved models/model___1.h5\n",
      "Epoch 1/5\n",
      "375/375 [==============================] - 143s 381ms/step - loss: 4.7889 - accuracy: 0.0080\n",
      "Epoch 2/5\n",
      "375/375 [==============================] - 143s 381ms/step - loss: 4.7531 - accuracy: 0.0127\n",
      "Epoch 3/5\n",
      "375/375 [==============================] - 142s 379ms/step - loss: 4.5107 - accuracy: 0.0356\n",
      "Epoch 4/5\n",
      "375/375 [==============================] - 142s 379ms/step - loss: 4.3906 - accuracy: 0.0491\n",
      "Epoch 5/5\n",
      "375/375 [==============================] - 142s 379ms/step - loss: 4.2896 - accuracy: 0.0597\n",
      ">Saved models/model___2.h5\n",
      "Epoch 1/5\n",
      "375/375 [==============================] - 143s 381ms/step - loss: 4.7907 - accuracy: 0.0103\n",
      "Epoch 2/5\n",
      "375/375 [==============================] - 142s 379ms/step - loss: 4.7868 - accuracy: 0.0126\n",
      "Epoch 3/5\n",
      "375/375 [==============================] - 142s 379ms/step - loss: 4.7340 - accuracy: 0.0155\n",
      "Epoch 4/5\n",
      "375/375 [==============================] - 142s 379ms/step - loss: 4.4644 - accuracy: 0.0397\n",
      "Epoch 5/5\n",
      "375/375 [==============================] - 142s 378ms/step - loss: 4.2695 - accuracy: 0.0706\n",
      ">Saved models/model___3.h5\n",
      "Epoch 1/5\n",
      "375/375 [==============================] - 142s 378ms/step - loss: 4.7909 - accuracy: 0.0056\n",
      "Epoch 2/5\n",
      "375/375 [==============================] - 142s 378ms/step - loss: 4.7878 - accuracy: 0.0062\n",
      "Epoch 3/5\n",
      "375/375 [==============================] - 141s 377ms/step - loss: 4.7589 - accuracy: 0.0145\n",
      "Epoch 4/5\n",
      "375/375 [==============================] - 141s 376ms/step - loss: 4.5493 - accuracy: 0.0365\n",
      "Epoch 5/5\n",
      "375/375 [==============================] - 140s 375ms/step - loss: 4.4872 - accuracy: 0.0391\n",
      ">Saved models/model___4.h5\n",
      "Epoch 1/5\n",
      "375/375 [==============================] - 142s 378ms/step - loss: 4.7909 - accuracy: 0.0066\n",
      "Epoch 2/5\n",
      "375/375 [==============================] - 141s 376ms/step - loss: 4.7831 - accuracy: 0.0076\n",
      "Epoch 3/5\n",
      "375/375 [==============================] - 142s 377ms/step - loss: 4.6633 - accuracy: 0.0221\n",
      "Epoch 4/5\n",
      "375/375 [==============================] - 141s 375ms/step - loss: 4.4896 - accuracy: 0.0361\n",
      "Epoch 5/5\n",
      "375/375 [==============================] - 141s 376ms/step - loss: 4.3630 - accuracy: 0.0536\n",
      ">Saved models/model___5.h5\n",
      ">loaded models/model___1.h5\n",
      ">loaded models/model___2.h5\n",
      ">loaded models/model___3.h5\n",
      ">loaded models/model___4.h5\n",
      ">loaded models/model___5.h5\n",
      "Loaded 5 models\n",
      "269/269 [==============================] - 30s 112ms/step - loss: 4.6756 - accuracy: 0.0312\n",
      "Model Accuracy: 0.031\n",
      "269/269 [==============================] - 28s 104ms/step - loss: 4.5681 - accuracy: 0.0390\n",
      "Model Accuracy: 0.039\n",
      "269/269 [==============================] - 29s 108ms/step - loss: 4.5347 - accuracy: 0.0434\n",
      "Model Accuracy: 0.043\n",
      "269/269 [==============================] - 28s 102ms/step - loss: 4.5764 - accuracy: 0.0361\n",
      "Model Accuracy: 0.036\n",
      "269/269 [==============================] - 29s 106ms/step - loss: 4.4922 - accuracy: 0.0393\n",
      "Model Accuracy: 0.039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christopherfiaschetti/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/christopherfiaschetti/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacked Test Accuracy: 0.074\n"
     ]
    }
   ],
   "source": [
    "#makedirs('models')\n",
    "\n",
    "## fit and save models\n",
    "n_members = 5\n",
    "for i in range(n_members):\n",
    "    # fit model\n",
    "    model = fit_model(X_train, y_train)\n",
    "    # save model\n",
    "    filename = 'models/model___' + str(i + 1) + '.h5'\n",
    "    model.save(filename)\n",
    "    print('>Saved %s' % filename)\n",
    "\n",
    "all_simple_models = load_all_models(n_members)\n",
    "print('Loaded %d models' % len(all_simple_models))\n",
    "\n",
    "for model in all_simple_models:\n",
    "    #y_test_enc = to_categorical(y_test_numbers)\n",
    "    _, acc = model.evaluate(X_test, y_test, verbose=1)\n",
    "    print('Model Accuracy: %.3f' % acc)\n",
    "\n",
    "# Use 2 aforementioned functions to create dataset shape to fit ensemble model, and then fit ensemble model\n",
    "simple_model_ensemble = fit_stacked_model(all_simple_models, X_test, np.array(y_test_numbers))\n",
    "\n",
    "# Make predictions using fit ensemble model\n",
    "yhat_simple = stacked_prediction(all_simple_models, simple_model_ensemble, X_test)\n",
    "# Print accuracy of ensemble model\n",
    "acc_ensemble = accuracy_score(np.array(y_test_numbers), yhat_simple)\n",
    "print('Stacked Test Accuracy: %.3f' % acc_ensemble)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the test set accuracy of each individual model was between 3% and 4% (these were trained for 5 times as long as the sample \"handmade\" models shown at the beginning of the code, allowing for slight improvement from their ~0.9% random guessing accuracy), while the ensemble model has a test accuracy of 7.4%, approximately two times as high as the performance of the individual models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will create an ensemble model using 5 individual models that were each trained on a random fifth of the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "375/375 [==============================] - 142s 377ms/step - loss: 4.7899 - accuracy: 0.0096\n",
      "Epoch 2/5\n",
      "375/375 [==============================] - 141s 376ms/step - loss: 4.7875 - accuracy: 0.0072\n",
      "Epoch 3/5\n",
      "375/375 [==============================] - 141s 377ms/step - loss: 4.6903 - accuracy: 0.0208\n",
      "Epoch 4/5\n",
      "375/375 [==============================] - 142s 378ms/step - loss: 4.5077 - accuracy: 0.0405\n",
      "Epoch 5/5\n",
      "375/375 [==============================] - 141s 376ms/step - loss: 4.3861 - accuracy: 0.0518\n",
      ">Saved models/model_sub_1.h5\n",
      "Epoch 1/5\n",
      "375/375 [==============================] - 143s 381ms/step - loss: 4.7893 - accuracy: 0.0086\n",
      "Epoch 2/5\n",
      "375/375 [==============================] - 142s 379ms/step - loss: 4.7799 - accuracy: 0.0127\n",
      "Epoch 3/5\n",
      "375/375 [==============================] - 142s 379ms/step - loss: 4.7626 - accuracy: 0.0125\n",
      "Epoch 4/5\n",
      "375/375 [==============================] - 143s 381ms/step - loss: 4.6041 - accuracy: 0.0261\n",
      "Epoch 5/5\n",
      "375/375 [==============================] - 142s 379ms/step - loss: 4.4952 - accuracy: 0.0382\n",
      ">Saved models/model_sub_2.h5\n",
      "Epoch 1/5\n",
      "375/375 [==============================] - 143s 381ms/step - loss: 4.7886 - accuracy: 0.0079\n",
      "Epoch 2/5\n",
      "375/375 [==============================] - 143s 382ms/step - loss: 4.7395 - accuracy: 0.0145\n",
      "Epoch 3/5\n",
      "375/375 [==============================] - 143s 381ms/step - loss: 4.5188 - accuracy: 0.0361\n",
      "Epoch 4/5\n",
      "375/375 [==============================] - 143s 382ms/step - loss: 4.3560 - accuracy: 0.0581\n",
      "Epoch 5/5\n",
      "375/375 [==============================] - 143s 381ms/step - loss: 4.2227 - accuracy: 0.0737\n",
      ">Saved models/model_sub_3.h5\n",
      "Epoch 1/5\n",
      "375/375 [==============================] - 143s 379ms/step - loss: 4.7901 - accuracy: 0.0086\n",
      "Epoch 2/5\n",
      "375/375 [==============================] - 142s 379ms/step - loss: 4.6680 - accuracy: 0.0223\n",
      "Epoch 3/5\n",
      "375/375 [==============================] - 143s 380ms/step - loss: 4.4290 - accuracy: 0.0452\n",
      "Epoch 4/5\n",
      "375/375 [==============================] - 142s 379ms/step - loss: 4.3088 - accuracy: 0.0635\n",
      "Epoch 5/5\n",
      "375/375 [==============================] - 143s 381ms/step - loss: 4.2003 - accuracy: 0.0781\n",
      ">Saved models/model_sub_4.h5\n",
      "Epoch 1/5\n",
      "375/375 [==============================] - 143s 381ms/step - loss: 4.7888 - accuracy: 0.0072\n",
      "Epoch 2/5\n",
      "375/375 [==============================] - 142s 380ms/step - loss: 4.7880 - accuracy: 0.0064\n",
      "Epoch 3/5\n",
      "375/375 [==============================] - 142s 380ms/step - loss: 4.7679 - accuracy: 0.0114\n",
      "Epoch 4/5\n",
      "375/375 [==============================] - 142s 380ms/step - loss: 4.5058 - accuracy: 0.0391\n",
      "Epoch 5/5\n",
      "375/375 [==============================] - 143s 381ms/step - loss: 4.3590 - accuracy: 0.0523\n",
      ">Saved models/model_sub_5.h5\n",
      ">loaded models/model_sub_1.h5\n",
      ">loaded models/model_sub_2.h5\n",
      ">loaded models/model_sub_3.h5\n",
      ">loaded models/model_sub_4.h5\n",
      ">loaded models/model_sub_5.h5\n",
      "Loaded 5 models\n",
      "269/269 [==============================] - 30s 111ms/step - loss: 4.6061 - accuracy: 0.0314\n",
      "Model Accuracy: 0.031\n",
      "269/269 [==============================] - 28s 105ms/step - loss: 4.5952 - accuracy: 0.0321\n",
      "Model Accuracy: 0.032\n",
      "269/269 [==============================] - 28s 105ms/step - loss: 4.6905 - accuracy: 0.0348\n",
      "Model Accuracy: 0.035\n",
      "269/269 [==============================] - 29s 109ms/step - loss: 4.5134 - accuracy: 0.0443\n",
      "Model Accuracy: 0.044\n",
      "269/269 [==============================] - 29s 107ms/step - loss: 4.5477 - accuracy: 0.0372\n",
      "Model Accuracy: 0.037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christopherfiaschetti/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/christopherfiaschetti/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacked Test Accuracy: 0.054\n"
     ]
    }
   ],
   "source": [
    "## fit and save models\n",
    "n_members = 5\n",
    "for i in range(n_members):\n",
    "    # Define first and last indices of section of training set that will be used to train this simple network\n",
    "    start = int((i/n_members)*len(X_train))\n",
    "    stop = int(((i+1)/n_members)*len(X_train))\n",
    "    model = fit_model(X_train[start:stop,:,:,:], y_train[start:stop,:])\n",
    "    # save model\n",
    "    filename = 'models/model_sub_' + str(i + 1) + '.h5'\n",
    "    model.save(filename)\n",
    "    print('>Saved %s' % filename)\n",
    "\n",
    "\n",
    "all_simple_sub_models = load_all_sub_models(n_members)\n",
    "print('Loaded %d models' % len(all_simple_sub_models))\n",
    "\n",
    "\n",
    "for model in all_simple_sub_models:\n",
    "    #y_test_enc = to_categorical(y_test_numbers)\n",
    "    _, acc = model.evaluate(X_test, y_test, verbose=1)\n",
    "    print('Model Accuracy: %.3f' % acc)\n",
    "\n",
    "# Use 2 aforementioned functions to create dataset shape to fit ensemble model, and then fit ensemble model\n",
    "simple_sub_model_ensemble = fit_stacked_model(all_simple_sub_models, X_test, np.array(y_test_numbers))\n",
    "\n",
    "\n",
    "# Make predictions using fit ensemble model\n",
    "yhat_simple_sub = stacked_prediction(all_simple_sub_models, simple_sub_model_ensemble, X_test)\n",
    "# Print accuracy of ensemble model\n",
    "acc_ensemble_sub = accuracy_score(np.array(y_test_numbers), yhat_simple_sub)\n",
    "print('Stacked Test Accuracy: %.3f' % acc_ensemble_sub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the training times and individual models' performances were very similar to those exhibited in the ensemble model above (where each sub-model was trained on the full dataset), the ensemble's test set accuracy was nearly two percentage points lower than that of the ensemble composed of sub-models trained on the full dataset. Nevertheless, we still see that the ensembling of the models improves on their individual performances by 1%-2%.\n",
    "\n",
    "Both of these handmade ensembles' performance metrics pale in comparison to the performance of the transfer learning ensembles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = load_model('models/model_3.h5')\n",
    "model4 = load_model('models/model_4.h5')\n",
    "model5 = load_model('models/model_5.h5')\n",
    "model6 = load_model('models/emnet.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotmodel(model, X_train, y_train, X_test, y_test):\n",
    "    temp = []\n",
    "    for i in [3000,6000,8000,10000,12000]:\n",
    "        indices = random.sample(list(range(len(X_train))),i)\n",
    "        a = X_train[indices,:,:,:]\n",
    "        l = y_train[indices,:]\n",
    "        model.fit(a,l, epochs=5, steps_per_epoch= 20, batch_size=64)\n",
    "        temp.append(model.evaluate(X_test,y_test)[0])\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "20/20 [==============================] - 81s 4s/step - loss: 3.6123 - categorical_accuracy: 0.1953\n",
      "Epoch 2/5\n",
      "20/20 [==============================] - 82s 4s/step - loss: 3.5610 - categorical_accuracy: 0.2266\n",
      "Epoch 3/5\n",
      "20/20 [==============================] - 81s 4s/step - loss: 3.4960 - categorical_accuracy: 0.2358\n",
      "Epoch 4/5\n",
      "20/20 [==============================] - 81s 4s/step - loss: 3.4101 - categorical_accuracy: 0.2461\n",
      "Epoch 5/5\n",
      "20/20 [==============================] - 81s 4s/step - loss: 3.4464 - categorical_accuracy: 0.2421\n",
      "269/269 [==============================] - 547s 2s/step - loss: 3.7084 - categorical_accuracy: 0.1691\n",
      "Epoch 1/5\n",
      "20/20 [==============================] - 80s 4s/step - loss: 3.4171 - categorical_accuracy: 0.2453\n",
      "Epoch 2/5\n",
      "20/20 [==============================] - 82s 4s/step - loss: 3.4467 - categorical_accuracy: 0.2477\n",
      "Epoch 3/5\n",
      "20/20 [==============================] - 81s 4s/step - loss: 3.4613 - categorical_accuracy: 0.2438\n",
      "Epoch 4/5\n",
      "20/20 [==============================] - 80s 4s/step - loss: 3.4311 - categorical_accuracy: 0.2430\n",
      "Epoch 5/5\n",
      "20/20 [==============================] - 81s 4s/step - loss: 3.4180 - categorical_accuracy: 0.2437\n",
      "269/269 [==============================] - 555s 2s/step - loss: 3.6510 - categorical_accuracy: 0.1803\n",
      "Epoch 1/5\n",
      "20/20 [==============================] - 79s 4s/step - loss: 3.3354 - categorical_accuracy: 0.2617\n",
      "Epoch 2/5\n",
      "20/20 [==============================] - 81s 4s/step - loss: 3.2961 - categorical_accuracy: 0.2703\n",
      "Epoch 3/5\n",
      "20/20 [==============================] - 81s 4s/step - loss: 3.2991 - categorical_accuracy: 0.2711\n",
      "Epoch 4/5\n",
      "20/20 [==============================] - 80s 4s/step - loss: 3.3167 - categorical_accuracy: 0.2586\n",
      "Epoch 5/5\n",
      "20/20 [==============================] - 82s 4s/step - loss: 3.3880 - categorical_accuracy: 0.2461\n",
      "269/269 [==============================] - 545s 2s/step - loss: 3.5837 - categorical_accuracy: 0.1892\n",
      "Epoch 1/5\n",
      "20/20 [==============================] - 81s 4s/step - loss: 3.2593 - categorical_accuracy: 0.2898\n",
      "Epoch 2/5\n",
      "20/20 [==============================] - 82s 4s/step - loss: 3.2152 - categorical_accuracy: 0.2828\n",
      "Epoch 3/5\n",
      "20/20 [==============================] - 83s 4s/step - loss: 3.2364 - categorical_accuracy: 0.2945\n",
      "Epoch 4/5\n",
      "20/20 [==============================] - 81s 4s/step - loss: 3.2444 - categorical_accuracy: 0.2727\n",
      "Epoch 5/5\n",
      "20/20 [==============================] - 82s 4s/step - loss: 3.2282 - categorical_accuracy: 0.2898\n",
      "269/269 [==============================] - 548s 2s/step - loss: 3.5415 - categorical_accuracy: 0.1949\n",
      "Epoch 1/5\n",
      "20/20 [==============================] - 88s 4s/step - loss: 3.1272 - categorical_accuracy: 0.3117\n",
      "Epoch 2/5\n",
      "20/20 [==============================] - 82s 4s/step - loss: 3.1385 - categorical_accuracy: 0.3031\n",
      "Epoch 3/5\n",
      "20/20 [==============================] - 83s 4s/step - loss: 3.1574 - categorical_accuracy: 0.2984\n",
      "Epoch 4/5\n",
      "20/20 [==============================] - 83s 4s/step - loss: 3.1456 - categorical_accuracy: 0.3000\n",
      "Epoch 5/5\n",
      "20/20 [==============================] - 81s 4s/step - loss: 3.1299 - categorical_accuracy: 0.2906\n",
      "269/269 [==============================] - 548s 2s/step - loss: 3.4843 - categorical_accuracy: 0.2021\n",
      "Epoch 1/5\n",
      "20/20 [==============================] - 41s 2s/step - loss: 0.7144 - categorical_accuracy: 0.8078\n",
      "Epoch 2/5\n",
      "20/20 [==============================] - 41s 2s/step - loss: 0.6123 - categorical_accuracy: 0.8336\n",
      "Epoch 3/5\n",
      "20/20 [==============================] - 41s 2s/step - loss: 0.3877 - categorical_accuracy: 0.8742\n",
      "Epoch 4/5\n",
      "20/20 [==============================] - 41s 2s/step - loss: 0.3225 - categorical_accuracy: 0.8945\n",
      "Epoch 5/5\n",
      "20/20 [==============================] - 41s 2s/step - loss: 0.2775 - categorical_accuracy: 0.9119\n",
      "269/269 [==============================] - 274s 1s/step - loss: 1.0518 - categorical_accuracy: 0.7471\n",
      "Epoch 1/5\n",
      "20/20 [==============================] - 41s 2s/step - loss: 0.4850 - categorical_accuracy: 0.8609\n",
      "Epoch 2/5\n",
      "20/20 [==============================] - 43s 2s/step - loss: 0.4753 - categorical_accuracy: 0.8656\n",
      "Epoch 3/5\n",
      "20/20 [==============================] - 44s 2s/step - loss: 0.5239 - categorical_accuracy: 0.8406\n",
      "Epoch 4/5\n",
      "20/20 [==============================] - 43s 2s/step - loss: 0.5481 - categorical_accuracy: 0.8398\n",
      "Epoch 5/5\n",
      "20/20 [==============================] - 43s 2s/step - loss: 0.4891 - categorical_accuracy: 0.8394\n",
      "269/269 [==============================] - 289s 1s/step - loss: 1.0748 - categorical_accuracy: 0.7414\n",
      "Epoch 1/5\n",
      "20/20 [==============================] - 42s 2s/step - loss: 0.4534 - categorical_accuracy: 0.8672\n",
      "Epoch 2/5\n",
      "20/20 [==============================] - 42s 2s/step - loss: 0.4331 - categorical_accuracy: 0.8742\n",
      "Epoch 3/5\n",
      "20/20 [==============================] - 42s 2s/step - loss: 0.4448 - categorical_accuracy: 0.8586\n",
      "Epoch 4/5\n",
      "20/20 [==============================] - 42s 2s/step - loss: 0.4495 - categorical_accuracy: 0.8539\n",
      "Epoch 5/5\n",
      "20/20 [==============================] - 42s 2s/step - loss: 0.4201 - categorical_accuracy: 0.8641\n",
      "269/269 [==============================] - 276s 1s/step - loss: 1.0658 - categorical_accuracy: 0.7473\n",
      "Epoch 1/5\n",
      "20/20 [==============================] - 43s 2s/step - loss: 0.3807 - categorical_accuracy: 0.8789\n",
      "Epoch 2/5\n",
      "20/20 [==============================] - 44s 2s/step - loss: 0.3301 - categorical_accuracy: 0.9016\n",
      "Epoch 3/5\n",
      "20/20 [==============================] - 43s 2s/step - loss: 0.3958 - categorical_accuracy: 0.8859\n",
      "Epoch 4/5\n",
      "20/20 [==============================] - 44s 2s/step - loss: 0.3875 - categorical_accuracy: 0.8781\n",
      "Epoch 5/5\n",
      "20/20 [==============================] - 43s 2s/step - loss: 0.3679 - categorical_accuracy: 0.8734\n",
      "269/269 [==============================] - 295s 1s/step - loss: 1.0784 - categorical_accuracy: 0.7432\n",
      "Epoch 1/5\n",
      "20/20 [==============================] - 45s 2s/step - loss: 0.3068 - categorical_accuracy: 0.8984\n",
      "Epoch 2/5\n",
      "20/20 [==============================] - 46s 2s/step - loss: 0.2846 - categorical_accuracy: 0.9180\n",
      "Epoch 3/5\n",
      "20/20 [==============================] - 45s 2s/step - loss: 0.3129 - categorical_accuracy: 0.8930\n",
      "Epoch 4/5\n",
      "20/20 [==============================] - 44s 2s/step - loss: 0.2974 - categorical_accuracy: 0.8977\n",
      "Epoch 5/5\n",
      "20/20 [==============================] - 43s 2s/step - loss: 0.3583 - categorical_accuracy: 0.8867\n",
      "269/269 [==============================] - 291s 1s/step - loss: 1.0482 - categorical_accuracy: 0.7528\n",
      "Epoch 1/5\n",
      "20/20 [==============================] - 39s 2s/step - loss: 3.4292 - categorical_accuracy: 0.6711\n",
      "Epoch 2/5\n",
      "20/20 [==============================] - 36s 2s/step - loss: 2.5627 - categorical_accuracy: 0.7227\n",
      "Epoch 3/5\n",
      "20/20 [==============================] - 36s 2s/step - loss: 2.0662 - categorical_accuracy: 0.7492\n",
      "Epoch 4/5\n",
      "20/20 [==============================] - 36s 2s/step - loss: 1.1378 - categorical_accuracy: 0.8273\n",
      "Epoch 5/5\n",
      "20/20 [==============================] - 35s 2s/step - loss: 0.9905 - categorical_accuracy: 0.8467\n",
      "269/269 [==============================] - 202s 746ms/step - loss: 4.9858 - categorical_accuracy: 0.5829\n",
      "Epoch 1/5\n",
      "20/20 [==============================] - 36s 2s/step - loss: 1.8703 - categorical_accuracy: 0.7727\n",
      "Epoch 2/5\n",
      "20/20 [==============================] - 37s 2s/step - loss: 2.1099 - categorical_accuracy: 0.7617\n",
      "Epoch 3/5\n",
      "20/20 [==============================] - 37s 2s/step - loss: 2.2418 - categorical_accuracy: 0.7563\n",
      "Epoch 4/5\n",
      "20/20 [==============================] - 36s 2s/step - loss: 2.1163 - categorical_accuracy: 0.7422\n",
      "Epoch 5/5\n",
      "20/20 [==============================] - 36s 2s/step - loss: 2.0290 - categorical_accuracy: 0.7476\n",
      "269/269 [==============================] - 208s 774ms/step - loss: 5.0479 - categorical_accuracy: 0.5767\n",
      "Epoch 1/5\n",
      "20/20 [==============================] - 38s 2s/step - loss: 1.7092 - categorical_accuracy: 0.7828\n",
      "Epoch 2/5\n",
      "20/20 [==============================] - 37s 2s/step - loss: 1.8649 - categorical_accuracy: 0.7633\n",
      "Epoch 3/5\n",
      "20/20 [==============================] - 37s 2s/step - loss: 1.9250 - categorical_accuracy: 0.7648\n",
      "Epoch 4/5\n",
      "20/20 [==============================] - 37s 2s/step - loss: 1.7937 - categorical_accuracy: 0.7672\n",
      "Epoch 5/5\n",
      "20/20 [==============================] - 38s 2s/step - loss: 2.0028 - categorical_accuracy: 0.7414\n",
      "269/269 [==============================] - 214s 796ms/step - loss: 5.0560 - categorical_accuracy: 0.5843\n",
      "Epoch 1/5\n",
      "20/20 [==============================] - 37s 2s/step - loss: 1.1468 - categorical_accuracy: 0.8398\n",
      "Epoch 2/5\n",
      "20/20 [==============================] - 37s 2s/step - loss: 1.2107 - categorical_accuracy: 0.8313\n",
      "Epoch 3/5\n",
      "20/20 [==============================] - 38s 2s/step - loss: 1.4172 - categorical_accuracy: 0.8062\n",
      "Epoch 4/5\n",
      "20/20 [==============================] - 38s 2s/step - loss: 1.4893 - categorical_accuracy: 0.7914\n",
      "Epoch 5/5\n",
      "20/20 [==============================] - 37s 2s/step - loss: 1.4424 - categorical_accuracy: 0.8031\n",
      "269/269 [==============================] - 216s 803ms/step - loss: 4.9516 - categorical_accuracy: 0.5807\n",
      "Epoch 1/5\n",
      "20/20 [==============================] - 35s 2s/step - loss: 1.1413 - categorical_accuracy: 0.8352\n",
      "Epoch 2/5\n",
      "20/20 [==============================] - 36s 2s/step - loss: 1.1128 - categorical_accuracy: 0.8313\n",
      "Epoch 3/5\n",
      "20/20 [==============================] - 39s 2s/step - loss: 1.1253 - categorical_accuracy: 0.8359\n",
      "Epoch 4/5\n",
      "20/20 [==============================] - 37s 2s/step - loss: 1.0471 - categorical_accuracy: 0.8430\n",
      "Epoch 5/5\n",
      "20/20 [==============================] - 37s 2s/step - loss: 1.4090 - categorical_accuracy: 0.8031\n",
      "269/269 [==============================] - 203s 755ms/step - loss: 5.3838 - categorical_accuracy: 0.5692\n",
      "Epoch 1/5\n",
      "20/20 [==============================] - 28s 1s/step - loss: 7.0234 - accuracy: 0.0039\n",
      "Epoch 2/5\n",
      "20/20 [==============================] - 24s 1s/step - loss: 5.8829 - accuracy: 0.0094\n",
      "Epoch 3/5\n",
      "20/20 [==============================] - 24s 1s/step - loss: 5.6538 - accuracy: 0.0086\n",
      "Epoch 4/5\n",
      "20/20 [==============================] - 24s 1s/step - loss: 5.5107 - accuracy: 0.0070\n",
      "Epoch 5/5\n",
      "20/20 [==============================] - 24s 1s/step - loss: 5.6658 - accuracy: 0.0071\n",
      "269/269 [==============================] - 135s 493ms/step - loss: 5.4534 - accuracy: 0.0092\n",
      "Epoch 1/5\n",
      "20/20 [==============================] - 25s 1s/step - loss: 5.5711 - accuracy: 0.0047\n",
      "Epoch 2/5\n",
      "20/20 [==============================] - 24s 1s/step - loss: 5.6603 - accuracy: 0.0063\n",
      "Epoch 3/5\n",
      "20/20 [==============================] - 24s 1s/step - loss: 5.6909 - accuracy: 0.0070\n",
      "Epoch 4/5\n",
      "20/20 [==============================] - 24s 1s/step - loss: 5.5910 - accuracy: 0.0094\n",
      "Epoch 5/5\n",
      "20/20 [==============================] - 24s 1s/step - loss: 5.5921 - accuracy: 0.0087\n",
      "269/269 [==============================] - 132s 491ms/step - loss: 5.4631 - accuracy: 0.0058\n",
      "Epoch 1/5\n",
      "20/20 [==============================] - 26s 1s/step - loss: 5.5888 - accuracy: 0.0063\n",
      "Epoch 2/5\n",
      "20/20 [==============================] - 24s 1s/step - loss: 5.5946 - accuracy: 0.0055\n",
      "Epoch 3/5\n",
      "20/20 [==============================] - 24s 1s/step - loss: 5.6732 - accuracy: 0.0078\n",
      "Epoch 4/5\n",
      "20/20 [==============================] - 24s 1s/step - loss: 5.6757 - accuracy: 0.0086\n",
      "Epoch 5/5\n",
      "20/20 [==============================] - 24s 1s/step - loss: 5.6407 - accuracy: 0.0086\n",
      "269/269 [==============================] - 133s 492ms/step - loss: 5.6567 - accuracy: 0.0080\n",
      "Epoch 1/5\n",
      "20/20 [==============================] - 27s 1s/step - loss: 5.6879 - accuracy: 0.0070\n",
      "Epoch 2/5\n",
      "20/20 [==============================] - 25s 1s/step - loss: 5.6495 - accuracy: 0.0086\n",
      "Epoch 3/5\n",
      "20/20 [==============================] - 25s 1s/step - loss: 5.6495 - accuracy: 0.0078\n",
      "Epoch 4/5\n",
      "20/20 [==============================] - 25s 1s/step - loss: 5.6672 - accuracy: 0.0063\n",
      "Epoch 5/5\n",
      "20/20 [==============================] - 25s 1s/step - loss: 5.5464 - accuracy: 0.0086\n",
      "269/269 [==============================] - 134s 497ms/step - loss: 5.4409 - accuracy: 0.0111\n",
      "Epoch 1/5\n",
      "20/20 [==============================] - 29s 1s/step - loss: 5.6132 - accuracy: 0.0055\n",
      "Epoch 2/5\n",
      "20/20 [==============================] - 26s 1s/step - loss: 5.7460 - accuracy: 0.0047\n",
      "Epoch 3/5\n",
      "20/20 [==============================] - 26s 1s/step - loss: 5.8157 - accuracy: 0.0086\n",
      "Epoch 4/5\n",
      "20/20 [==============================] - 26s 1s/step - loss: 5.6283 - accuracy: 0.0086\n",
      "Epoch 5/5\n",
      "20/20 [==============================] - 26s 1s/step - loss: 5.6808 - accuracy: 0.0055\n",
      "269/269 [==============================] - 142s 527ms/step - loss: 5.7460 - accuracy: 0.0069\n"
     ]
    }
   ],
   "source": [
    "#Plot test accuracies for different train set size\n",
    "size = [3000,6000,8000,10000,12000]\n",
    "res1 = plotmodel(model3, X_train, y_train, X_test, y_test)\n",
    "res2 = plotmodel(model4, X_train, y_train, X_test, y_test)\n",
    "res3 = plotmodel(model5, X_train, y_train, X_test, y_test)\n",
    "res4 = plotmodel(model6, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fb11a56ba58>]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(res1, size)\n",
    "plt.plot(res2, size)\n",
    "plt.plot(res3, size)\n",
    "plt.plot(res4, size)\n",
    "plt.xlabel('Loss - Categorical Cross-Entropy')\n",
    "plt.ylabel('Training Set Size')\n",
    "plt.title('Loss vs. Training Set Size')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X axis is categorical cross-entropy loss, Y axis is training set size. Red is Efficient Net, Green is VGG, Blue is ResNet, and Yellow is Xception (our project writeup contains a version of this plot with the axes labeled and an accompanying legend; the kernel continued to die and we therefore had to leave this as is). We see minimal improvement as the training set size increases, which matches our comparison of the \"handmade\" ensembles' individual sub-model performances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotmodelacc(model, X_train, y_train, X_test, y_test):\n",
    "    temp = []\n",
    "    for i in [3000,6000,8000,10000,12000]:\n",
    "        indices = random.sample(list(range(len(X_train))),i)\n",
    "        a = X_train[indices,:,:,:]\n",
    "        l = y_train[indices,:]\n",
    "        model.fit(a,l, epochs=5, steps_per_epoch= 20, batch_size=64)\n",
    "        temp.append(model.evaluate(X_test,y_test)[1])\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "20/20 [==============================] - 75s 4s/step - loss: 3.0670 - categorical_accuracy: 0.3180\n",
      "Epoch 2/5\n",
      "20/20 [==============================] - 87s 4s/step - loss: 3.0699 - categorical_accuracy: 0.3102\n",
      "Epoch 3/5\n",
      "20/20 [==============================] - 82s 4s/step - loss: 3.0265 - categorical_accuracy: 0.3208\n",
      "Epoch 4/5\n",
      "20/20 [==============================] - 83s 4s/step - loss: 2.9854 - categorical_accuracy: 0.3320\n",
      "Epoch 5/5\n",
      "20/20 [==============================] - 82s 4s/step - loss: 2.9721 - categorical_accuracy: 0.3451\n",
      "269/269 [==============================] - 549s 2s/step - loss: 3.4627 - categorical_accuracy: 0.2124\n",
      "Epoch 1/5\n",
      "20/20 [==============================] - 81s 4s/step - loss: 3.0056 - categorical_accuracy: 0.3281\n",
      "Epoch 2/5\n",
      "20/20 [==============================] - 83s 4s/step - loss: 3.0117 - categorical_accuracy: 0.3359\n",
      "Epoch 3/5\n",
      "20/20 [==============================] - 81s 4s/step - loss: 3.0214 - categorical_accuracy: 0.3180\n",
      "Epoch 4/5\n",
      "20/20 [==============================] - 81s 4s/step - loss: 3.0177 - categorical_accuracy: 0.3266\n",
      "Epoch 5/5\n",
      "20/20 [==============================] - 81s 4s/step - loss: 3.0249 - categorical_accuracy: 0.3259\n",
      "269/269 [==============================] - 547s 2s/step - loss: 3.4034 - categorical_accuracy: 0.2108\n",
      "Epoch 1/5\n",
      "20/20 [==============================] - 85s 4s/step - loss: 2.9418 - categorical_accuracy: 0.3438\n",
      "Epoch 2/5\n",
      "20/20 [==============================] - 82s 4s/step - loss: 2.9403 - categorical_accuracy: 0.3406\n",
      "Epoch 3/5\n",
      "20/20 [==============================] - 81s 4s/step - loss: 2.9239 - categorical_accuracy: 0.3422\n",
      "Epoch 4/5\n",
      "20/20 [==============================] - 84s 4s/step - loss: 2.9436 - categorical_accuracy: 0.3281\n",
      "Epoch 5/5\n",
      "20/20 [==============================] - 84s 4s/step - loss: 2.9909 - categorical_accuracy: 0.3352\n",
      "269/269 [==============================] - 550s 2s/step - loss: 3.3684 - categorical_accuracy: 0.2226\n",
      "Epoch 1/5\n",
      "20/20 [==============================] - 80s 4s/step - loss: 2.9001 - categorical_accuracy: 0.3586\n",
      "Epoch 2/5\n",
      "20/20 [==============================] - 81s 4s/step - loss: 2.9092 - categorical_accuracy: 0.3383\n",
      "Epoch 3/5\n",
      "20/20 [==============================] - 81s 4s/step - loss: 2.8599 - categorical_accuracy: 0.3391\n",
      "Epoch 4/5\n",
      "20/20 [==============================] - 81s 4s/step - loss: 2.9014 - categorical_accuracy: 0.3508\n",
      "Epoch 5/5\n",
      "20/20 [==============================] - 82s 4s/step - loss: 2.8627 - categorical_accuracy: 0.3688\n",
      "269/269 [==============================] - 538s 2s/step - loss: 3.3275 - categorical_accuracy: 0.2244\n",
      "Epoch 1/5\n",
      "20/20 [==============================] - 79s 4s/step - loss: 2.8130 - categorical_accuracy: 0.3672\n",
      "Epoch 2/5\n",
      "20/20 [==============================] - 81s 4s/step - loss: 2.7939 - categorical_accuracy: 0.3609\n",
      "Epoch 3/5\n",
      "20/20 [==============================] - 81s 4s/step - loss: 2.7888 - categorical_accuracy: 0.3719\n",
      "Epoch 4/5\n",
      "20/20 [==============================] - 82s 4s/step - loss: 2.8627 - categorical_accuracy: 0.3469\n",
      "Epoch 5/5\n",
      "20/20 [==============================] - 82s 4s/step - loss: 2.8170 - categorical_accuracy: 0.3508\n",
      "269/269 [==============================] - 547s 2s/step - loss: 3.2794 - categorical_accuracy: 0.2346\n",
      "Epoch 1/5\n",
      "20/20 [==============================] - 44s 2s/step - loss: 0.3124 - categorical_accuracy: 0.9016\n",
      "Epoch 2/5\n",
      "20/20 [==============================] - 43s 2s/step - loss: 0.2911 - categorical_accuracy: 0.9078\n",
      "Epoch 3/5\n",
      "20/20 [==============================] - 43s 2s/step - loss: 0.1953 - categorical_accuracy: 0.9403\n",
      "Epoch 4/5\n",
      "20/20 [==============================] - 44s 2s/step - loss: 0.1646 - categorical_accuracy: 0.9469\n",
      "Epoch 5/5\n",
      "20/20 [==============================] - 43s 2s/step - loss: 0.1258 - categorical_accuracy: 0.9654\n",
      "269/269 [==============================] - 293s 1s/step - loss: 1.0469 - categorical_accuracy: 0.7513\n",
      "Epoch 1/5\n",
      "20/20 [==============================] - 46s 2s/step - loss: 0.2765 - categorical_accuracy: 0.9141\n",
      "Epoch 2/5\n",
      "20/20 [==============================] - 44s 2s/step - loss: 0.2536 - categorical_accuracy: 0.9125\n",
      "Epoch 3/5\n",
      "20/20 [==============================] - 45s 2s/step - loss: 0.2385 - categorical_accuracy: 0.9227\n",
      "Epoch 4/5\n",
      "20/20 [==============================] - 43s 2s/step - loss: 0.2678 - categorical_accuracy: 0.9156\n",
      "Epoch 5/5\n",
      "20/20 [==============================] - 44s 2s/step - loss: 0.2490 - categorical_accuracy: 0.9153\n",
      "269/269 [==============================] - 291s 1s/step - loss: 1.0729 - categorical_accuracy: 0.7493\n",
      "Epoch 1/5\n",
      "20/20 [==============================] - 39s 2s/step - loss: 0.2043 - categorical_accuracy: 0.9359\n",
      "Epoch 2/5\n",
      "20/20 [==============================] - 42s 2s/step - loss: 0.2254 - categorical_accuracy: 0.9281\n",
      "Epoch 3/5\n",
      "20/20 [==============================] - 43s 2s/step - loss: 0.2525 - categorical_accuracy: 0.9227\n",
      "Epoch 4/5\n",
      "20/20 [==============================] - 42s 2s/step - loss: 0.2664 - categorical_accuracy: 0.9203\n",
      "Epoch 5/5\n",
      "20/20 [==============================] - 42s 2s/step - loss: 0.2522 - categorical_accuracy: 0.9242\n",
      "269/269 [==============================] - 284s 1s/step - loss: 1.1072 - categorical_accuracy: 0.7434\n",
      "Epoch 1/5\n",
      "20/20 [==============================] - 41s 2s/step - loss: 0.2146 - categorical_accuracy: 0.9312\n",
      "Epoch 2/5\n",
      "20/20 [==============================] - 44s 2s/step - loss: 0.1836 - categorical_accuracy: 0.9430\n",
      "Epoch 3/5\n",
      "20/20 [==============================] - 42s 2s/step - loss: 0.2036 - categorical_accuracy: 0.9375\n",
      "Epoch 4/5\n",
      "20/20 [==============================] - 43s 2s/step - loss: 0.1995 - categorical_accuracy: 0.9352\n",
      "Epoch 5/5\n",
      "20/20 [==============================] - 44s 2s/step - loss: 0.2030 - categorical_accuracy: 0.9297\n",
      "269/269 [==============================] - 292s 1s/step - loss: 1.0840 - categorical_accuracy: 0.7472\n",
      "Epoch 1/5\n",
      "20/20 [==============================] - 48s 2s/step - loss: 0.1628 - categorical_accuracy: 0.9484\n",
      "Epoch 2/5\n",
      "20/20 [==============================] - 44s 2s/step - loss: 0.1720 - categorical_accuracy: 0.9484\n",
      "Epoch 3/5\n",
      "20/20 [==============================] - 45s 2s/step - loss: 0.1953 - categorical_accuracy: 0.9359\n",
      "Epoch 4/5\n",
      "20/20 [==============================] - 45s 2s/step - loss: 0.1929 - categorical_accuracy: 0.9352\n",
      "Epoch 5/5\n",
      "20/20 [==============================] - 43s 2s/step - loss: 0.1792 - categorical_accuracy: 0.9469\n",
      "269/269 [==============================] - 304s 1s/step - loss: 1.0925 - categorical_accuracy: 0.7493\n",
      "Epoch 1/5\n",
      "20/20 [==============================] - 37s 2s/step - loss: 1.0077 - categorical_accuracy: 0.8305\n",
      "Epoch 2/5\n",
      "20/20 [==============================] - 37s 2s/step - loss: 0.9120 - categorical_accuracy: 0.8500\n",
      "Epoch 3/5\n",
      "20/20 [==============================] - 37s 2s/step - loss: 0.6789 - categorical_accuracy: 0.8726\n",
      "Epoch 4/5\n",
      "20/20 [==============================] - 36s 2s/step - loss: 0.4007 - categorical_accuracy: 0.9195\n",
      "Epoch 5/5\n",
      "20/20 [==============================] - 36s 2s/step - loss: 0.2987 - categorical_accuracy: 0.9308\n",
      "269/269 [==============================] - 212s 789ms/step - loss: 4.9046 - categorical_accuracy: 0.5930\n",
      "Epoch 1/5\n",
      "20/20 [==============================] - 35s 2s/step - loss: 0.6938 - categorical_accuracy: 0.8766\n",
      "Epoch 2/5\n",
      "20/20 [==============================] - 35s 2s/step - loss: 0.9085 - categorical_accuracy: 0.8484\n",
      "Epoch 3/5\n",
      "20/20 [==============================] - 37s 2s/step - loss: 0.8630 - categorical_accuracy: 0.8625\n",
      "Epoch 4/5\n",
      "20/20 [==============================] - 37s 2s/step - loss: 0.7915 - categorical_accuracy: 0.8531\n",
      "Epoch 5/5\n",
      "20/20 [==============================] - 37s 2s/step - loss: 0.9779 - categorical_accuracy: 0.8354\n",
      "269/269 [==============================] - 204s 758ms/step - loss: 4.9344 - categorical_accuracy: 0.5922\n",
      "Epoch 1/5\n",
      "20/20 [==============================] - 35s 2s/step - loss: 0.5055 - categorical_accuracy: 0.8922\n",
      "Epoch 2/5\n",
      "20/20 [==============================] - 37s 2s/step - loss: 0.6791 - categorical_accuracy: 0.8773\n",
      "Epoch 3/5\n",
      "20/20 [==============================] - 37s 2s/step - loss: 0.6163 - categorical_accuracy: 0.8805\n",
      "Epoch 4/5\n",
      "20/20 [==============================] - 37s 2s/step - loss: 0.8575 - categorical_accuracy: 0.8664\n",
      "Epoch 5/5\n",
      "20/20 [==============================] - 37s 2s/step - loss: 0.9639 - categorical_accuracy: 0.8492\n",
      "269/269 [==============================] - 210s 780ms/step - loss: 5.1301 - categorical_accuracy: 0.5914\n",
      "Epoch 1/5\n",
      "20/20 [==============================] - 36s 2s/step - loss: 0.6114 - categorical_accuracy: 0.8914\n",
      "Epoch 2/5\n",
      "20/20 [==============================] - 37s 2s/step - loss: 0.6419 - categorical_accuracy: 0.8773\n",
      "Epoch 3/5\n",
      "20/20 [==============================] - 38s 2s/step - loss: 0.5639 - categorical_accuracy: 0.8969\n",
      "Epoch 4/5\n",
      "20/20 [==============================] - 40s 2s/step - loss: 0.7293 - categorical_accuracy: 0.8773\n",
      "Epoch 5/5\n",
      "20/20 [==============================] - 37s 2s/step - loss: 0.7349 - categorical_accuracy: 0.8797\n",
      "269/269 [==============================] - 215s 798ms/step - loss: 5.5597 - categorical_accuracy: 0.5857\n",
      "Epoch 1/5\n",
      "20/20 [==============================] - 37s 2s/step - loss: 0.6532 - categorical_accuracy: 0.8875\n",
      "Epoch 2/5\n",
      "20/20 [==============================] - 39s 2s/step - loss: 0.5443 - categorical_accuracy: 0.9000\n",
      "Epoch 3/5\n",
      "20/20 [==============================] - 40s 2s/step - loss: 0.5333 - categorical_accuracy: 0.9023\n",
      "Epoch 4/5\n",
      "20/20 [==============================] - 38s 2s/step - loss: 0.5504 - categorical_accuracy: 0.8922\n",
      "Epoch 5/5\n",
      "20/20 [==============================] - 38s 2s/step - loss: 0.6904 - categorical_accuracy: 0.8867\n",
      "269/269 [==============================] - 219s 815ms/step - loss: 5.3738 - categorical_accuracy: 0.5874\n",
      "Epoch 1/5\n",
      "20/20 [==============================] - 24s 1s/step - loss: 7.1025 - accuracy: 0.0070\n",
      "Epoch 2/5\n",
      "20/20 [==============================] - 24s 1s/step - loss: 5.9215 - accuracy: 0.0102\n",
      "Epoch 3/5\n",
      "20/20 [==============================] - 24s 1s/step - loss: 5.6618 - accuracy: 0.0102\n",
      "Epoch 4/5\n",
      "20/20 [==============================] - 24s 1s/step - loss: 5.6424 - accuracy: 0.0047\n",
      "Epoch 5/5\n",
      "20/20 [==============================] - 24s 1s/step - loss: 5.4707 - accuracy: 0.0086\n",
      "269/269 [==============================] - 132s 491ms/step - loss: 5.6708 - accuracy: 0.0070\n",
      "Epoch 1/5\n",
      "20/20 [==============================] - 26s 1s/step - loss: 5.5098 - accuracy: 0.0094\n",
      "Epoch 2/5\n",
      "20/20 [==============================] - 25s 1s/step - loss: 5.7061 - accuracy: 0.0063\n",
      "Epoch 3/5\n",
      "20/20 [==============================] - 26s 1s/step - loss: 5.5997 - accuracy: 0.0156\n",
      "Epoch 4/5\n",
      "20/20 [==============================] - 26s 1s/step - loss: 5.6725 - accuracy: 0.0070\n",
      "Epoch 5/5\n",
      "20/20 [==============================] - 25s 1s/step - loss: 5.6499 - accuracy: 0.0071\n",
      "269/269 [==============================] - 140s 520ms/step - loss: 5.4852 - accuracy: 0.0103\n",
      "Epoch 1/5\n",
      "20/20 [==============================] - 30s 1s/step - loss: 5.6341 - accuracy: 0.0070\n",
      "Epoch 2/5\n",
      "20/20 [==============================] - 25s 1s/step - loss: 5.5081 - accuracy: 0.0102\n",
      "Epoch 3/5\n",
      "20/20 [==============================] - 26s 1s/step - loss: 5.6482 - accuracy: 0.0117\n",
      "Epoch 4/5\n",
      "20/20 [==============================] - 26s 1s/step - loss: 5.7063 - accuracy: 0.0070\n",
      "Epoch 5/5\n",
      "20/20 [==============================] - 26s 1s/step - loss: 5.5736 - accuracy: 0.0063\n",
      "269/269 [==============================] - 137s 507ms/step - loss: 5.5253 - accuracy: 0.0089\n",
      "Epoch 1/5\n",
      "20/20 [==============================] - 30s 1s/step - loss: 5.5678 - accuracy: 0.0031\n",
      "Epoch 2/5\n",
      "20/20 [==============================] - 26s 1s/step - loss: 5.6280 - accuracy: 0.0070\n",
      "Epoch 3/5\n",
      "20/20 [==============================] - 26s 1s/step - loss: 5.6165 - accuracy: 0.0102\n",
      "Epoch 4/5\n",
      "20/20 [==============================] - 26s 1s/step - loss: 5.5917 - accuracy: 0.0133\n",
      "Epoch 5/5\n",
      "20/20 [==============================] - 25s 1s/step - loss: 5.6589 - accuracy: 0.0078\n",
      "269/269 [==============================] - 139s 514ms/step - loss: 5.6182 - accuracy: 0.0075\n",
      "Epoch 1/5\n",
      "20/20 [==============================] - 27s 1s/step - loss: 5.6468 - accuracy: 0.0063\n",
      "Epoch 2/5\n",
      "20/20 [==============================] - 26s 1s/step - loss: 5.6231 - accuracy: 0.0109\n",
      "Epoch 3/5\n",
      "20/20 [==============================] - 27s 1s/step - loss: 5.5826 - accuracy: 0.0070\n",
      "Epoch 4/5\n",
      "20/20 [==============================] - 25s 1s/step - loss: 5.5786 - accuracy: 0.0094\n",
      "Epoch 5/5\n",
      "20/20 [==============================] - 26s 1s/step - loss: 5.6471 - accuracy: 0.0078\n",
      "269/269 [==============================] - 140s 521ms/step - loss: 5.5595 - accuracy: 0.0117\n"
     ]
    }
   ],
   "source": [
    "res1acc = plotmodelacc(model3, X_train, y_train, X_test, y_test)\n",
    "res2acc = plotmodelacc(model4, X_train, y_train, X_test, y_test)\n",
    "res3acc = plotmodelacc(model5, X_train, y_train, X_test, y_test)\n",
    "res4acc = plotmodelacc(mnet, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94/94 [==============================] - 292s 3s/step - loss: 20.1390 - categorical_accuracy: 0.0971 - val_loss: 8.2892 - val_categorical_accuracy: 0.3635\n",
      "94/94 [==============================] - 384s 4s/step - loss: 3.6275 - categorical_accuracy: 0.3586 - val_loss: 1.4227 - val_categorical_accuracy: 0.6517\n",
      "94/94 [==============================] - 756s 8s/step - loss: 5.2776 - categorical_accuracy: 0.0149 - val_loss: 4.7777 - val_categorical_accuracy: 0.0259\n",
      "94/94 [==============================] - 205s 2s/step - loss: 8.1123 - accuracy: 0.0093 - val_loss: 6.0406 - val_accuracy: 0.0082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christopherfiaschetti/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/christopherfiaschetti/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/Users/christopherfiaschetti/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/christopherfiaschetti/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188/188 [==============================] - 379s 2s/step - loss: 16.4266 - categorical_accuracy: 0.1735 - val_loss: 7.8048 - val_categorical_accuracy: 0.4326\n",
      "188/188 [==============================] - 503s 3s/step - loss: 2.9158 - categorical_accuracy: 0.4553 - val_loss: 1.2542 - val_categorical_accuracy: 0.6985\n",
      "188/188 [==============================] - 944s 5s/step - loss: 5.0638 - categorical_accuracy: 0.0135 - val_loss: 4.4578 - val_categorical_accuracy: 0.0548\n",
      "188/188 [==============================] - 272s 1s/step - loss: 7.4590 - accuracy: 0.0066 - val_loss: 6.0966 - val_accuracy: 0.0084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christopherfiaschetti/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/christopherfiaschetti/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/Users/christopherfiaschetti/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/christopherfiaschetti/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 395s 2s/step - loss: 14.5644 - categorical_accuracy: 0.2183 - val_loss: 6.6693 - val_categorical_accuracy: 0.5002\n",
      "250/250 [==============================] - 570s 2s/step - loss: 2.5886 - categorical_accuracy: 0.5009 - val_loss: 1.2872 - val_categorical_accuracy: 0.6983\n",
      "250/250 [==============================] - 1126s 5s/step - loss: 4.9929 - categorical_accuracy: 0.0252 - val_loss: 4.3661 - val_categorical_accuracy: 0.0612\n",
      "250/250 [==============================] - 306s 1s/step - loss: 6.7735 - accuracy: 0.0077 - val_loss: 5.8803 - val_accuracy: 0.0096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christopherfiaschetti/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/christopherfiaschetti/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/Users/christopherfiaschetti/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/christopherfiaschetti/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 471s 2s/step - loss: 13.4936 - categorical_accuracy: 0.2402 - val_loss: 7.0460 - val_categorical_accuracy: 0.4854\n",
      "313/313 [==============================] - 611s 2s/step - loss: 2.4327 - categorical_accuracy: 0.5191 - val_loss: 1.4282 - val_categorical_accuracy: 0.6787\n",
      "313/313 [==============================] - 1139s 4s/step - loss: 4.9487 - categorical_accuracy: 0.0218 - val_loss: 4.2157 - val_categorical_accuracy: 0.0804\n",
      "313/313 [==============================] - 334s 1s/step - loss: 6.9216 - accuracy: 0.0086 - val_loss: 5.9486 - val_accuracy: 0.0063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christopherfiaschetti/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/christopherfiaschetti/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/Users/christopherfiaschetti/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/christopherfiaschetti/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "375/375 [==============================] - 516s 1s/step - loss: 13.6368 - categorical_accuracy: 0.2624 - val_loss: 7.6738 - val_categorical_accuracy: 0.4911\n",
      "375/375 [==============================] - 680s 2s/step - loss: 2.2907 - categorical_accuracy: 0.5467 - val_loss: 1.3505 - val_categorical_accuracy: 0.7052\n",
      "375/375 [==============================] - 1299s 3s/step - loss: 4.8728 - categorical_accuracy: 0.0264 - val_loss: 4.0882 - val_categorical_accuracy: 0.0981\n",
      "375/375 [==============================] - 381s 1s/step - loss: 6.6528 - accuracy: 0.0085 - val_loss: 6.2193 - val_accuracy: 0.0061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christopherfiaschetti/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/christopherfiaschetti/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/Users/christopherfiaschetti/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/christopherfiaschetti/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "res5acc = []\n",
    "res6acc = []\n",
    "\n",
    "for i in [3000,6000,8000,10000,12000]:\n",
    "    \n",
    "    plotted_models3 = []\n",
    "    plotted_models4 = []\n",
    "    indices = random.sample(list(range(len(X_train))),i)\n",
    "    a = X_train[indices,:,:,:]\n",
    "    l = y_train[indices,:]\n",
    "    \n",
    "    conv_base = ResNet50V2(weights=\"imagenet\",include_top=False,input_shape=(200,200,3))\n",
    "    modela = Sequential()\n",
    "    conv_base.trainable=False\n",
    "    modela.add(conv_base)\n",
    "    modela.add(tfkl.GlobalMaxPool2D())\n",
    "    modela.add(tfkl.Dense(120,activation='softmax'))\n",
    "    modela.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['categorical_accuracy'])\n",
    "    modela.fit(a,l, epochs=1, batch_size=32, validation_data=(X_test, y_test))\n",
    "    plotted_models3.append(modela)\n",
    "    plotted_models4.append(modela)\n",
    "    \n",
    "    \n",
    "    conv_base = Xception(weights=\"imagenet\",include_top=False,input_shape=(200,200,3))\n",
    "    modelb = Sequential()\n",
    "    conv_base.trainable=False\n",
    "    modelb.add(conv_base)\n",
    "    modelb.add(tfkl.GlobalMaxPool2D())\n",
    "    modelb.add(tfkl.Dense(120,activation='softmax'))\n",
    "    modelb.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['categorical_accuracy'])\n",
    "    modelb.fit(a,l, epochs=1, batch_size=32, validation_data=(X_test, y_test))\n",
    "    plotted_models3.append(modelb)\n",
    "    plotted_models4.append(modelb)\n",
    "    \n",
    "\n",
    "    conv_base = VGG16(weights=\"imagenet\",include_top=False,input_shape=(200,200,3))\n",
    "    modelc = Sequential()\n",
    "    conv_base.trainable=False\n",
    "    modelc.add(conv_base)\n",
    "    modelc.add(tfkl.GlobalMaxPool2D())\n",
    "    modelc.add(tfkl.Dense(120,activation='softmax'))\n",
    "    modelc.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['categorical_accuracy'])\n",
    "    modelc.fit(a,l, epochs=1, batch_size=32, validation_data=(X_test, y_test))\n",
    "    plotted_models3.append(modelc)\n",
    "    plotted_models4.append(modelc)\n",
    "    \n",
    "    \n",
    "    efnet = Net(weights = \"imagenet\", include_top = False, input_shape = (200,200,3))\n",
    "    efnet.trainable = False\n",
    "    # Define transfer learning model\n",
    "    modeld = Sequential()\n",
    "    # Add Efficient Net weights to model\n",
    "    modeld.add(efnet)\n",
    "    # Add final dense 120-element layer that will contain softmax probabilities\n",
    "    modeld.add(tfkl.GlobalMaxPooling2D(name = \"gap\"))\n",
    "    modeld.add(tfkl.Flatten())\n",
    "    modeld.add(tfkl.Dense(120, activation='softmax'))\n",
    "    # Compile model\n",
    "    modeld.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "    modeld.fit(a,l, epochs=1, batch_size=32, validation_data=(X_test, y_test))\n",
    "    plotted_models4.append(modeld)\n",
    "    \n",
    "    \n",
    "    \n",
    "    plotted_model_ensemble3 = fit_stacked_model(plotted_models3, X_test, np.array(y_test_numbers))\n",
    "    plotted_model_ensemble4 = fit_stacked_model(plotted_models4, X_test, np.array(y_test_numbers))\n",
    "    \n",
    "    yhat3 = stacked_prediction(plotted_models3, plotted_model_ensemble3, X_test)\n",
    "    yhat4 = stacked_prediction(plotted_models4, plotted_model_ensemble4, X_test)\n",
    "    \n",
    "    acc3 = accuracy_score(np.array(y_test_numbers), yhat3)\n",
    "    acc4 = accuracy_score(np.array(y_test_numbers), yhat4)\n",
    "\n",
    "    res5acc.append(acc3)\n",
    "    res6acc.append(acc4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7faf3c5c7780>]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(res1acc, size)\n",
    "plt.plot(res2acc, size)\n",
    "plt.plot(res3acc, size)\n",
    "plt.plot(res4acc, size)\n",
    "plt.plot(res5acc, size)\n",
    "plt.plot(res6acc, size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this plot, X axis is test set accuracy, Y axis is training set size (and again, our project writeup contains a version of this plot with the axes labeled and an accompanying legend; the kernel continued to die and we therefore had to leave this as is). In this plot, the maroon line is both ensemble models, because as we saw, the addition of Efficient Net to the ensemble adds no value (orange is Xception along, Green is ResNet alone, Blue is VGG alone, and Red is Efficient Net alone). And again, we see minimal improvement as the training set size increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "590Project.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
